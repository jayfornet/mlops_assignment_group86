name: Simple Data Trigger - Auto MLOps Pipeline

# Super simple data monitoring that triggers MLOps pipeline when data changes
# Perfect for assignments - no webhooks, no complex setup required

on:
  schedule:
    # Check for data changes every 30 minutes (frequent for demo)
    - cron: '*/30 * * * *'
  workflow_dispatch:
    # Manual trigger for testing
    inputs:
      force_trigger:
        description: 'Force trigger even if no changes'
        type: boolean
        default: false

permissions:
  contents: write
  actions: write

jobs:
  check-and-trigger:
    runs-on: ubuntu-latest
    name: Check Data and Trigger MLOps
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.9
        
    - name: Install dependencies
      run: |
        pip install pandas numpy scikit-learn
        
    - name: Download and check data
      id: data_check
      run: |
        echo "üì• Downloading latest dataset..."
        python src/data/download_dataset.py
        
        echo "üîç Checking for data changes..."
        python << 'EOF'
        import pandas as pd
        import hashlib
        import json
        import os
        from datetime import datetime
        
        # File paths
        data_file = "data/california_housing.csv"
        hash_file = "data/last_hash.txt"
        
        # Calculate current data hash
        if os.path.exists(data_file):
            df = pd.read_csv(data_file)
            current_hash = hashlib.md5(df.to_string().encode()).hexdigest()
            
            # Read previous hash
            previous_hash = ""
            if os.path.exists(hash_file):
                with open(hash_file, 'r') as f:
                    previous_hash = f.read().strip()
            
            # Check if data changed
            data_changed = current_hash != previous_hash
            
            print(f"Current hash: {current_hash}")
            print(f"Previous hash: {previous_hash}")
            print(f"Data changed: {data_changed}")
            
            # Save current hash
            os.makedirs("data", exist_ok=True)
            with open(hash_file, 'w') as f:
                f.write(current_hash)
            
            # Set GitHub output
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write(f"data_changed={str(data_changed).lower()}\n")
                f.write(f"dataset_rows={len(df)}\n")
                f.write(f"dataset_cols={len(df.columns)}\n")
                
            # Basic validation
            if len(df) < 100:
                print("‚ùå Dataset too small!")
                exit(1)
            
            print(f"‚úÖ Dataset valid: {len(df)} rows, {len(df.columns)} columns")
        else:
            print("‚ùå Dataset not found!")
            exit(1)
        EOF
        
    - name: Commit hash file
      if: steps.data_check.outputs.data_changed == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/last_hash.txt
        git commit -m "Update data hash [skip ci]" || echo "No changes to commit"
        git push || echo "No changes to push"
        
    - name: Trigger MLOps Pipeline
      if: steps.data_check.outputs.data_changed == 'true' || github.event.inputs.force_trigger == 'true'
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          console.log('üöÄ Triggering MLOps Pipeline...');
          
          await github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'mlops-pipeline.yml',
            ref: 'main',
            inputs: {
              'triggered_by': 'data-trigger',
              'trigger_reason': 'Data update detected by monitoring'
            }
          });
          
          console.log('‚úÖ MLOps Pipeline triggered successfully!');
          
    - name: Report results
      run: |
        echo "üìä Data Monitoring Results"
        echo "========================="
        echo "Data Changed: ${{ steps.data_check.outputs.data_changed }}"
        echo "Dataset Rows: ${{ steps.data_check.outputs.dataset_rows }}"
        echo "Dataset Cols: ${{ steps.data_check.outputs.dataset_cols }}"
        echo "Pipeline Triggered: ${{ steps.data_check.outputs.data_changed == 'true' || github.event.inputs.force_trigger == 'true' }}"
        echo "Timestamp: $(date)"
